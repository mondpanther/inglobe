---
title: "Prepare persons data"
output: html_notebook
---

This notebook prepares the 
- person/firm level identifiers

```{r}

library(dplyr)
library(arrow)

```



```{r}

source("find_dropbox.R")

dropbox_root <- get_dropbox_path()  # works for personal and enterprise Dropbox
if (is.null(dropbox_root)) stop("Dropbox folder not found. Is Dropbox installed?")
message("Dropbox root: ", dropbox_root)

inglobebig=paste0(dropbox_root,"\\Apps\\iseapp\\inglobe")

```



### get address data from bigquery unless already stored locally....

```{r}
personfile=paste0(inglobebig,"\\person_address.parquet")
if (!file.exists(personfile)) {
  library(bigrquery)
  library(DBI)
  
  # Set your project ID
  project_id <- "patbis"
  
  # Option 1: Direct table download
  # Specify dataset and table name
  dataset <- "inglobe"
  table <- "person_address_extract"
  
  # Download the entire table
  df <- bq_table_download(
    bq_table(project_id, dataset, table)
  )
  library(arrow)
  write_parquet(df,personfile)
}

persons_address=read_parquet(personfile)
head(persons_address)

```
# Get the persons data..

```{r}
filen=paste0(inglobebig,"\\persons.parquet")
if (!file.exists(filen)) {
  library(bigrquery)
  library(DBI)

  # Set your project ID
  project_id <- "patbis"

  # Download selected columns from tls206_person
  sql <- "
    SELECT
      person_id,
      person_name,
      person_ctry_code,
      psn_name,
      han_name,
      psn_sector
    FROM `patbis.fromPATSTAT2021.tls206_person`
  "

  tb <- bq_project_query(project_id, sql)
  df <- bq_table_download(tb)

  library(arrow)
  write_parquet(df,filen)
}

persons=read_parquet(filen)
head(persons)

```

### Get top 100 technology companies by R&D spending (EU Industrial R&D Investment Scoreboard)

```{r}
library(readxl)
library(dplyr)
library(arrow)

top100file <- paste0(inglobebig, "\\top100_rd.parquet")
if (!file.exists(top100file)) {
  # Download the 2025 EU Industrial R&D Investment Scoreboard (World 2000)
  # Source: https://iri.jrc.ec.europa.eu/scoreboard/2025-eu-industrial-rd-investment-scoreboard
  scoreboard_url <- "https://iri.jrc.ec.europa.eu/sites/default/files/contentype/scoreboard/2025-12/SB2025_World2000_2.xlsx"

  tmpfile <- tempfile(fileext = ".xlsx")
  download.file(scoreboard_url, tmpfile, mode = "wb")

  # Read the excel file
  sb <- read_excel(tmpfile, sheet = 1)
  unlink(tmpfile)

  message("Columns: ", paste(names(sb), collapse = ", "))

  write_parquet(sb, top100file)
}

top_rd <- read_parquet(top100file)
head(top_rd)





```


### get docdb and application id

```{r}

library(fst)

filen=paste0(inglobebig,"\\data\\appln_docdb.fst")
if (!file.exists(filen)) {
  library(bigrquery)
  library(DBI)
  
  # Set your project ID
  project_id <- "patbis"
  
  # Option 1: Direct table download
  # Specify dataset and table name
  dataset <- "fromPATSTAT2021"
  table <- "tls201_appln"
  
  # Download the entire table
  #df <- bq_table_download(
  #  bq_table(project_id, dataset, table),
  #  selected_fields = c("appln_id", "appln_filing_year", "docdb_familily_id")
  #)
  
  
  
  # Use SQL to select specific columns
  sql <- "
    SELECT 
      appln_id, 
      appln_filing_year, 
      docdb_family_id,
      appln_nr,
      appln_auth,
    FROM `patbis.fromPATSTAT2021.tls201_appln`
    WHERE appln_filing_year>=2009
  "
  
  # Run query and download results
  tb <- bq_project_query(project_id, sql)
  df <- bq_table_download(tb)
  
  df=df %>% distinct()
  
  library(fst)
  write_fst(df,filen,compress=100)
}



#### let's create a file with priority pats....
  library(fst)
  df=read_fst(filen)
  library(data.table)
  # Convert to data.table if not already
  setDT(df)
  # Method A: Using .SD
  df <- df[order(docdb_family_id, appln_filing_year), .SD[1], by = docdb_family_id]
  
  
  ISEAPP_PATH_LOCAL=paste0(dropbox_root,"\\Apps\\iseapp\\")
  write_fst(df,paste0(ISEAPP_PATH_LOCAL,"first_appli.fst"),compress=100)
  
  
  pat2009t2018=df %>% filter(appln_filing_year>=2009 & appln_filing_year<=2018)
  pat2009t2018=pat2009t2018 %>% select(docdb_family_id)

```


## Get person data

```{r}


project_id <- "patbis"
dataset <- "inglobe"

# Define list of tables to download
tables_to_download <- c( "holders","inventors",  "inventor_countries", "holder_countries")
#tables_to_download <- c( "inventor_countries", "holder_countries")
# Loop through each table
for (table in tables_to_download) {
  #table="holders"
  # Create local filename
  local_file <- paste0(inglobebig, "\\data\\", table, ".parquet")
  
  # Check if file already exists
  if (!file.exists(local_file)) {
    message("Downloading ", table, "...")
    
    # Download the table
    table_data <- bq_table_download(
      bq_table(project_id, dataset, table)
    )
    
    # Save as parquet
    write_parquet(table_data, local_file)
    message("Saved to ", local_file)
  } else {
    message("File already exists: ", local_file)
  }
}


```







### a little check.... are high value patents MNE ones?

```{r}
library(arrow)
# Define list of tables to read
tables_to_read <- c("inventors","holders", "inventor_countries", "holder_countries")

# Loop through each table and read into a dataframe
for (table in tables_to_read) {
  # Create filename
  local_file <- paste0(inglobebig, "\\data\\", table, ".parquet")
  
  # Read parquet file and assign to variable with table name
  if (file.exists(local_file)) {
    assign(table, read_parquet(local_file))
    message("Loaded ", table, " (", nrow(get(table)), " rows)")
  } else {
    warning("File not found: ", local_file)
  }
}


source("prepdatahelper.r")


holderORinventor=holder_countries %>% mutate(type="holder") %>%
  bind_rows(inventor_countries %>% mutate(type="inventor")) %>% 
  rename(iso2=person_ctry_code) %>% 
  inner_join(country_income)

test=holderORinventor %>% inner_join(pat2009t2018) %>% group_by(iso2,type) %>% summarize(n()) 

test2=test %>% filter(iso2=="NA")
```

#### focus on holders only

```{r}

holders_persons=holders %>% distinct(person_id) %>% inner_join(persons)



```

### Match EU R&D Scoreboard companies to PATSTAT person_ids

**Workflow overview:**

1. Use the OpenAI API to generate known aliases, subsidiaries, and former
   names for each of the ~2000 scoreboard companies. This creates an
   expanded alias table (e.g. ALPHABET → GOOGLE, META → FACEBOOK).
2. Upload the alias table to BigQuery.
3. Match aliases against PATSTAT `han_name` using exact match on cleaned
   names + `EDIT_DISTANCE` for close variants.
4. Expand matched `han_id` → `person_id`, restricted to holders.
5. Download and cache the final mapping locally.

#### Step 1: Generate company aliases via OpenAI

```{r}
library(httr2)
library(jsonlite)
library(dplyr)
library(stringr)
library(arrow)

aliasfile <- paste0(inglobebig, "\\scoreboard_aliases.parquet")

if (!file.exists(aliasfile)) {

  api_key <- Sys.getenv("OPENAI_API_KEY")
  if (nchar(api_key) == 0) stop("Set OPENAI_API_KEY in .Renviron and restart R")

  companies <- top_rd %>% pull(Company) %>% unique()

  # Process in batches of 20 to keep prompt + response manageable
  batch_size <- 20
  batches    <- split(companies, ceiling(seq_along(companies) / batch_size))

  all_aliases <- list()

  for (i in seq_along(batches)) {
    batch <- batches[[i]]
    message("Processing batch ", i, " / ", length(batches),
            " (", length(batch), " companies)")

    company_list <- paste(batch, collapse = "\n")

    prompt <- paste0(
      "For each company listed below, provide ALL known names that this ",
      "company might appear under in patent filings. Include:\n",
      "- The company name itself\n",
      "- Major subsidiaries that file patents\n",
      "- Former names (e.g. FACEBOOK for META)\n",
      "- Common abbreviations or trading names\n",
      "- Regional variants (e.g. SAMSUNG ELECTRONICS, SAMSUNG SDI)\n\n",
      "Return ONLY a JSON array of objects, each with:\n",
      "  - \"company\": the original name exactly as given\n",
      "  - \"aliases\": array of strings (patent filing names, UPPERCASE)\n\n",
      "Do NOT include legal suffixes (Inc, Ltd, Corp, GmbH, etc.) in aliases.\n",
      "Focus on names that would appear in patent databases.\n",
      "Return valid JSON only, no markdown, no commentary.\n\n",
      "Companies:\n", company_list
    )

    resp <- request("https://api.openai.com/v1/chat/completions") %>%
      req_headers(
        "Authorization" = paste("Bearer", api_key),
        "Content-Type"  = "application/json"
      ) %>%
      req_body_json(list(
        model       = "gpt-4o-mini",
        temperature = 0,
        messages    = list(
          list(role = "system",
               content = "You are a corporate research assistant with deep knowledge of multinational companies, their subsidiaries, former names, and patent filing entities. Return only valid JSON."),
          list(role = "user", content = prompt)
        )
      )) %>%
      req_timeout(120) %>%
      req_retry(max_tries = 3, backoff = ~ 5) %>%
      req_perform()

    content <- resp_body_json(resp)
    reply   <- content$choices[[1]]$message$content

    # Clean up response: strip markdown fences if present
    reply <- str_replace_all(reply, "^```json\\s*|```\\s*$", "")

    parsed <- tryCatch(
      fromJSON(reply, simplifyDataFrame = FALSE),
      error = function(e) {
        message("  JSON parse error in batch ", i, ": ", e$message)
        message("  Raw reply (first 500 chars): ", substr(reply, 1, 500))
        return(NULL)
      }
    )

    if (!is.null(parsed)) {
      for (item in parsed) {
        company_name <- item$company
        aliases      <- unlist(item$aliases)
        if (length(aliases) > 0) {
          all_aliases[[length(all_aliases) + 1]] <- tibble(
            company_raw = company_name,
            alias       = toupper(trimws(aliases))
          )
        }
      }
    }

    # Respect rate limits
    Sys.sleep(1)
  }

  alias_table <- bind_rows(all_aliases) %>% distinct()
  message("Generated ", nrow(alias_table), " aliases for ",
          n_distinct(alias_table$company_raw), " companies")

  write_parquet(alias_table, aliasfile)
}

alias_table <- read_parquet(aliasfile)
message("Alias table: ", nrow(alias_table), " aliases for ",
        n_distinct(alias_table$company_raw), " companies")

# Quick look
alias_table %>% group_by(company_raw) %>%
  summarise(n_aliases = n(), .groups = "drop") %>%
  arrange(desc(n_aliases)) %>% head(10)

```

#### Step 2: Clean aliases and upload to BigQuery

```{r}
library(bigrquery)
library(dplyr)
library(stringr)
library(arrow)

project_id <- "patbis"
dataset    <- "inglobe"

# Clean alias names: strip legal suffixes and punctuation (same treatment
# we will apply to han_name on the BigQuery side)
alias_clean <- alias_table %>%
  mutate(
    alias_clean = alias %>%
      toupper() %>%
      str_replace_all("[.,;:!?'\"\\-]", " ") %>%
      str_replace_all(
        "\\b(PLC|LTD|LIMITED|INC|INCORPORATED|CORP|CORPORATION|CO|COMPANY|GROUP|HOLDINGS|HOLDING|AG|GMBH|SA|SPA|NV|BV|SE|AS|KK|OYJ|AB|A/S|SRL|LLC|LP|& CO)\\b",
        ""
      ) %>%
      str_replace_all("\\s+", " ") %>%
      str_trim()
  ) %>%
  filter(nchar(alias_clean) >= 2) %>%
  distinct()

# Upload to BigQuery
bq_table_upload(
  x = bq_table(project_id, dataset, "scoreboard_aliases"),
  values = alias_clean,
  write_disposition = "WRITE_TRUNCATE"
)
message("Uploaded ", nrow(alias_clean), " cleaned aliases to BigQuery")

```


#### Step 3: Match aliases against PATSTAT han_name on BigQuery

We use exact match on cleaned names plus `EDIT_DISTANCE` for close
variants. The normalized edit distance (edit_distance / max_length)
must be below 0.15 to qualify, which catches typos and minor
abbreviation differences but rejects unrelated names.

```{r}
matchfile <- paste0(inglobebig, "\\scoreboard_person_mapping.parquet")

if (!file.exists(matchfile)) {

  match_sql <- "
    -- Clean han_name: strip legal suffixes, uppercase, trim
    WITH clean_han AS (
      SELECT DISTINCT
        han_id,
        han_name,
        TRIM(REGEXP_REPLACE(
          UPPER(han_name),
          r'\\b(PLC|LTD|LIMITED|INC|INCORPORATED|CORP|CORPORATION|CO|COMPANY|GROUP|HOLDINGS|HOLDING|AG|GMBH|SA|SPA|NV|BV|SE|AS|KK|OYJ|AB|SRL|LLC|LP|& CO)\\b',
          ''
        )) AS han_clean
      FROM `patbis.fromPATSTAT2021.tls206_person`
      WHERE psn_sector = 'COMPANY'
        AND han_name IS NOT NULL
        AND TRIM(han_name) != ''
    ),

    -- Stage 1: exact match on cleaned names
    exact_matches AS (
      SELECT
        a.company_raw,
        a.alias,
        a.alias_clean,
        ch.han_id,
        ch.han_name,
        ch.han_clean,
        'exact' AS match_type
      FROM `patbis.inglobe.scoreboard_aliases` a
      INNER JOIN clean_han ch
        ON UPPER(TRIM(ch.han_clean)) = UPPER(TRIM(a.alias_clean))
    ),

    -- Stage 2: EDIT_DISTANCE for close variants (only for aliases not
    -- yet matched exactly). We restrict candidates by requiring the
    -- first 3 characters to match (cheap blocking) and the lengths
    -- to be within 3 of each other, then apply EDIT_DISTANCE.
    unmatched_aliases AS (
      SELECT a.*
      FROM `patbis.inglobe.scoreboard_aliases` a
      LEFT JOIN exact_matches em
        ON a.company_raw = em.company_raw
        AND a.alias_clean = em.alias_clean
      WHERE em.company_raw IS NULL
        AND LENGTH(TRIM(a.alias_clean)) >= 4
    ),

    fuzzy_matches AS (
      SELECT
        ua.company_raw,
        ua.alias,
        ua.alias_clean,
        ch.han_id,
        ch.han_name,
        ch.han_clean,
        'fuzzy' AS match_type
      FROM unmatched_aliases ua
      CROSS JOIN clean_han ch
      WHERE
        -- Blocking: first 3 chars must match
        SUBSTR(UPPER(TRIM(ch.han_clean)), 1, 3)
          = SUBSTR(UPPER(TRIM(ua.alias_clean)), 1, 3)
        -- Length must be within 3 characters
        AND ABS(LENGTH(TRIM(ch.han_clean)) - LENGTH(TRIM(ua.alias_clean))) <= 3
        -- Normalized edit distance < 0.15
        AND EDIT_DISTANCE(
              UPPER(TRIM(ch.han_clean)),
              UPPER(TRIM(ua.alias_clean)),
              max_distance => 5
            ) < GREATEST(LENGTH(TRIM(ch.han_clean)), LENGTH(TRIM(ua.alias_clean))) * 0.15
    ),

    -- Combine
    all_matches AS (
      SELECT * FROM exact_matches
      UNION ALL
      SELECT * FROM fuzzy_matches
    )

    -- Expand to person_id level, restricted to holders
    SELECT DISTINCT
      m.company_raw,
      m.alias,
      m.match_type,
      m.han_id,
      m.han_name,
      p.person_id,
      p.person_name,
      p.person_ctry_code
    FROM all_matches m
    INNER JOIN `patbis.fromPATSTAT2021.tls206_person` p
      ON m.han_id = p.han_id
    INNER JOIN (
      SELECT DISTINCT person_id
      FROM `patbis.inglobe.holders`
    ) h
      ON p.person_id = h.person_id
  "

  message("Running matching query on BigQuery...")
  tb <- bq_project_query(project_id, match_sql)
  scoreboard_person_mapping <- bq_table_download(tb)

  write_parquet(scoreboard_person_mapping, matchfile)
  message("Saved ", nrow(scoreboard_person_mapping), " rows to ", matchfile)
}

scoreboard_person_mapping <- read_parquet(matchfile)


n_distinct(scoreboard_person_mapping$company_raw)
test=scoreboard_person_mapping %>% distinct(company_raw)
```

#### Step 4: Local fuzzy matching fallback for remaining unmatched companies

```{r}
library(stringdist)

# Which scoreboard companies got zero matches?
matched_companies   <- unique(scoreboard_person_mapping$company_raw)
all_companies       <- unique(top_rd$Company)
unmatched_companies <- setdiff(all_companies, matched_companies)

message("Matched: ", length(matched_companies), " / ", length(all_companies))
message("Unmatched: ", length(unmatched_companies))

if (length(unmatched_companies) > 0) {

  # Download distinct company han_names from BigQuery (small table)
  han_sql <- "
    SELECT DISTINCT han_id, han_name
    FROM `patbis.fromPATSTAT2021.tls206_person`
    WHERE psn_sector = 'COMPANY'
      AND han_name IS NOT NULL
      AND TRIM(han_name) != ''
  "
  tb2 <- bq_project_query(project_id, han_sql)
  distinct_han <- bq_table_download(tb2)
  message("Downloaded ", nrow(distinct_han), " distinct company han_names")

  # Clean name helper
  clean_name <- function(x) {
    x %>% toupper() %>%
      str_replace_all("[.,;:!?'\"\\-]", " ") %>%
      str_replace_all(
        "\\b(PLC|LTD|LIMITED|INC|INCORPORATED|CORP|CORPORATION|CO|COMPANY|GROUP|HOLDINGS|HOLDING|AG|GMBH|SA|SPA|NV|BV|SE|AS|KK|OYJ|AB|A/S|SRL|LLC|LP|& CO)\\b",
        ""
      ) %>%
      str_replace_all("\\s+", " ") %>%
      str_trim()
  }

  # Build a search table that retains the link back to company_raw
  search_df <- bind_rows(
    # The company names themselves
    tibble(company_raw = unmatched_companies,
           alias       = unmatched_companies),
    # Their aliases
    alias_table %>% filter(company_raw %in% unmatched_companies)
  ) %>%
    distinct() %>%
    mutate(alias_clean = clean_name(alias))

  han_cleaned <- clean_name(distinct_han$han_name)

  message("Fuzzy matching ", nrow(search_df), " names against ",
          length(han_cleaned), " han_names...")

  # Jaro-Winkler fuzzy match
  match_idx <- amatch(search_df$alias_clean, han_cleaned,
                      method = "jw", maxDist = 0.03)

  # Collect results with company_raw linkage
  fuzzy_hits <- search_df %>%
    mutate(han_idx = match_idx) %>%
    filter(!is.na(han_idx)) %>%
    mutate(
      han_name = distinct_han$han_name[han_idx],
      han_id   = distinct_han$han_id[han_idx],
      jw_dist  = stringdist(alias_clean, han_cleaned[han_idx], method = "jw")
    )

  if (nrow(fuzzy_hits) > 0) {
    message("Local fuzzy matching found ", nrow(fuzzy_hits), " additional han_name matches")
    print(fuzzy_hits %>% select(company_raw, alias_clean, han_name, jw_dist))

    # Expand han_id -> person_id via BigQuery (upload matched han_ids, join there)
    fuzzy_han_ids <- fuzzy_hits %>% select(company_raw, alias, han_id, han_name) %>% distinct()

    bq_table_upload(
      x = bq_table(project_id, dataset, "fuzzy_han_matches"),
      values = fuzzy_han_ids,
      write_disposition = "WRITE_TRUNCATE"
    )

    expand_sql <- "
      SELECT DISTINCT
        f.company_raw,
        f.alias,
        'local_fuzzy' AS match_type,
        f.han_id,
        f.han_name,
        p.person_id,
        p.person_name,
        p.person_ctry_code
      FROM `patbis.inglobe.fuzzy_han_matches` f
      INNER JOIN `patbis.fromPATSTAT2021.tls206_person` p
        ON f.han_id = p.han_id
      INNER JOIN (
        SELECT DISTINCT person_id
        FROM `patbis.inglobe.holders`
      ) h
        ON p.person_id = h.person_id
    "
    tb3 <- bq_project_query(project_id, expand_sql)
    fuzzy_expanded <- bq_table_download(tb3)

    message("Fuzzy matches expanded to ", nrow(fuzzy_expanded), " person_id rows")

    # Merge into main mapping
    scoreboard_person_mapping <- bind_rows(
      scoreboard_person_mapping,
      fuzzy_expanded
    ) %>% distinct()

    # Re-save the combined mapping
    write_parquet(scoreboard_person_mapping, matchfile)
    message("Updated mapping saved: ", nrow(scoreboard_person_mapping), " total rows")

  } else {
    message("No additional fuzzy matches found")
  }
}

```


#### Step 5: Coverage report

```{r}
n_matched <- n_distinct(scoreboard_person_mapping$company_raw)
n_total   <- n_distinct(top_rd$Company)
message("Final coverage: ", n_matched, " / ", n_total, " scoreboard companies matched")

# Companies with most person_id matches (sanity check)
scoreboard_person_mapping %>%
  group_by(company_raw, match_type) %>%
  summarise(n_person_ids = n_distinct(person_id), .groups = "drop") %>%
  arrange(desc(n_person_ids)) %>%
  head(20)

# Unmatched companies
unmatched_final <- top_rd %>%
  distinct(Company) %>%
  anti_join(scoreboard_person_mapping, by = c("Company" = "company_raw"))
if (nrow(unmatched_final) > 0) {
  message("Still unmatched (", nrow(unmatched_final), " companies):")
  print(unmatched_final$Company)
}

```

#### Step 6: Map scoreboard companies to patent families (docdb_family_id)

Join `scoreboard_person_mapping` (company_raw → person_id) with both
`holders` and `inventors` to capture patent families where the company
appears as either assignee or inventor. Combine the unique mappings.

```{r}
library(dplyr)
library(arrow)

firmmap_file <- paste0(dropbox_root, "\\Apps\\iseapp\\firmmap.parquet")

person_ids <- scoreboard_person_mapping %>%
  select(company_raw, person_id) %>%
  distinct()

# Match via holders (company as patent assignee/owner)
firmmap_holders <- person_ids %>%
  inner_join(holders, by = "person_id") %>%
  select(company_raw, docdb_family_id) %>%
  distinct()
message("Via holders: ", nrow(firmmap_holders), " company-family pairs")

# Match via inventors (company as inventor — applies to corporate inventors)
firmmap_inventors <- person_ids %>%
  inner_join(inventors, by = "person_id") %>%
  select(company_raw, docdb_family_id) %>%
  distinct()
message("Via inventors: ", nrow(firmmap_inventors), " company-family pairs")

# Combine and deduplicate
firmmap <- bind_rows(firmmap_holders, firmmap_inventors) %>%
  distinct()

message("Combined firm map: ", nrow(firmmap), " rows mapping ",
        n_distinct(firmmap$company_raw), " companies to ",
        n_distinct(firmmap$docdb_family_id), " patent families")

write_parquet(firmmap, firmmap_file)
message("Saved to ", firmmap_file)

head(firmmap)

# Quick summary: patents per company
firmmap %>%
  group_by(company_raw) %>%
  summarise(n_families = n_distinct(docdb_family_id), .groups = "drop") %>%
  arrange(desc(n_families)) %>%
  head(20)


firmmap %>% distinct(docdb_family_id) %>% nrow()


```

#### Step 7: Create firm sector mapping

Map each company to its ICB3 sector classification from the scoreboard.

```{r}
library(dplyr)
library(arrow)

# Identify the ICB3 long column name (may vary across scoreboard editions)
icb3_col <- grep("ICB3.*long|ICB3_long", names(top_rd), value = TRUE, ignore.case = TRUE)
if (length(icb3_col) == 0) {
  message("Available columns: ", paste(names(top_rd), collapse = ", "))
  stop("Could not find ICB3 long column in top_rd")
}
message("Using column: ", icb3_col[1])

firmsectormap <- top_rd %>%
  select(company_raw = Company, sector = all_of(icb3_col[1]),
         RnD = `R&D (€million)`) %>%
  distinct() %>%
  filter(company_raw %in% unique(firmmap$company_raw))

message("Firm sector map: ", nrow(firmsectormap), " companies with sector classification")

sectormap_file <- paste0(dropbox_root, "\\Apps\\iseapp\\firmsectormap.parquet")
write_parquet(firmsectormap, sectormap_file)
message("Saved to ", sectormap_file)

# Summary by sector
firmsectormap %>%
  group_by(sector) %>%
  summarise(n_companies = n(), .groups = "drop") %>%
  arrange(desc(n_companies))


test=top_rd %>% filter(`ICB3 long`=="Electronic & Electrical Equipment") %>%  arrange(`ICB3 long`,`World rank`)

test %>% head(10) %>% pull(Company) %>% as.vector()
```


# Create dfnew: geocoded person locations

The following sections geocode person addresses via Mapbox, validate the results
against country boundaries, infer ISO2 codes from coordinates, and build `dfnew`
with fallback to capital city coordinates for unmatched entries.

### Geocode addresses via Mapbox

```{r}

library(arrow)
library(dplyr)
library(tidygeocoder)

df=read_parquet(paste0(inglobebig,"\\person_address.parquet"))

# Check if we have stored geocoded results previously
mboxfile=paste0(inglobebig,"\\data\\lat_longs_mapbox.parquet")
if (!file.exists(mboxfile)) {
  some_addresses=df %>% head(10) %>% mutate(addressstr=paste(person_address,person_ctry_code,sep=","))
  # geocode the addresses
  lat_longs_mapbox <- some_addresses %>%
    geocode(addressstr, method = 'mapbox',
            lat = latitude , long = longitude)
} else{
  lat_longs_mapbox=read_parquet(mboxfile)
}


recursive_mapbox=function(){
  df_filtered <- df %>%
    anti_join(lat_longs_mapbox, by = "person_id")

  df_filtered=df_filtered %>% mutate(rr=runif(1:n()))  %>% arrange(rr) %>% select(-rr)

  some_addresses=df_filtered %>% head(20000) %>%  mutate(addressstr=paste(person_address,person_ctry_code,sep=","))
  print(nrow(df_filtered))
  lat_longs_mapbox2 <- some_addresses %>%
    geocode(addressstr, method = 'mapbox', lat = latitude , long = longitude,full_results=F)


  lat_longs_mapbox_new=lat_longs_mapbox %>% bind_rows(lat_longs_mapbox2)
  return(lat_longs_mapbox_new)
}


lat_longs_mapbox=recursive_mapbox()
lat_longs_mapbox=lat_longs_mapbox %>% distinct()

write_parquet(lat_longs_mapbox,paste0(inglobebig,"\\data\\lat_longs_mapbox.parquet"))

```

### Reading back in from disk

```{r}
library(arrow)
lat_longs_mapbox=read_parquet(paste0(inglobebig,"\\data\\lat_longs_mapbox.parquet"))

```


#### Cleaning up
We examine if the points suggested fall within the country boundaries, which doesn't
always seem the case...


```{r}
test=lat_longs_mapbox %>% filter(nchar(person_address)<=2)
testdf=lat_longs_mapbox %>% filter(!is.na(person_ctry_code) & !is.na(longitude))

library(sf)
library(rnaturalearth)

# Get world boundaries
world <- ne_countries(scale = "medium", returnclass = "sf")

world <- world %>%
  mutate(iso_a2_fixed = case_when(
    name == "Taiwan" ~ "TW",
    name == "Kosovo" ~ "XK",
    iso_a2 == "-99" & !is.na(iso_a3) ~ iso_a3,  # Fallback to iso_a3
    TRUE ~ iso_a2
  ))

# Convert dataframe to spatial points
df_sf <- st_as_sf(testdf,
                  coords = c("longitude", "latitude"),
                  crs = 4326)

```



```{r}

# Detect number of cores
library(parallel)
n_cores <- detectCores() - 5  # Leave one core free

# Create cluster
cl <- makeCluster(n_cores)

# Export necessary objects to cluster
clusterExport(cl, c("df_sf", "world", "testdf"), envir = environment())
clusterEvalQ(cl, library(sf))

# Parallel processing
testdf$ok <- parSapply(cl, 1:nrow(testdf), function(i) {
  country_poly <- world[world$iso_a2 == testdf$person_ctry_code[i], ]
  if(nrow(country_poly) > 0) {
    return(st_intersects(df_sf[i,], country_poly, sparse = FALSE)[1,1])
  } else {
    return(FALSE)
  }
})


# Stop cluster
stopCluster(cl)


test=testdf %>% filter(person_ctry_code=="TW")


# correct some issues
testdf=testdf %>% mutate(ok=ifelse(person_ctry_code=="TW" ,T,ok), # correct some Chines corruption of the shapefile
                         ok=ifelse(nchar(gsub("\\s+", "", person_ctry_code))<2 ,T,ok) )


```



### Infer iso2

```{r}


library(sf)
library(rnaturalearth)
library(dplyr)

# Get world boundaries
world <- ne_countries(scale = "medium", returnclass = "sf")

# Initialize inferrediso2 column
lat_longs_mapbox <- lat_longs_mapbox %>%
  mutate(inferrediso2 = NA_character_)

# Filter to only rows with valid coordinates
valid_coords <- !is.na(lat_longs_mapbox$latitude) &
                !is.na(lat_longs_mapbox$longitude)

if(sum(valid_coords) > 0) {
  # Convert to spatial points (only valid coordinates)
  points_sf <- st_as_sf(lat_longs_mapbox[valid_coords, ],
                        coords = c("longitude", "latitude"),
                        crs = 4326)

  # Spatial join to find which country polygon contains each point
  points_with_country <- st_join(points_sf,
                                 world %>% select(iso_a2),
                                 join = st_intersects,
                                 left = TRUE)

  # Extract inferrediso2
  lat_longs_mapbox$inferrediso2[valid_coords] <- points_with_country$iso_a2

  # Special handling for Taiwan
  # Define Taiwan's bounding box with some buffer
  taiwan_bounds <- list(
    lat_min = 21.8,
    lat_max = 25.4,
    lon_min = 119.3,
    lon_max = 122.1
  )

  # Identify points in Taiwan region
  in_taiwan_region <- valid_coords &
                      lat_longs_mapbox$latitude >= taiwan_bounds$lat_min &
                      lat_longs_mapbox$latitude <= taiwan_bounds$lat_max &
                      lat_longs_mapbox$longitude >= taiwan_bounds$lon_min &
                      lat_longs_mapbox$longitude <= taiwan_bounds$lon_max

  # Override with TW for points in Taiwan region (regardless of what spatial join said)
  lat_longs_mapbox$inferrediso2[in_taiwan_region] <- "TW"
}


```

### Clean up some more...
```{r}

lat_longs_mapbox$inferrediso2[lat_longs_mapbox$inferrediso2=="-99"|lat_longs_mapbox$inferrediso2==""] <- NA

lat_longs_mapbox= lat_longs_mapbox %>% mutate(person_ctry_code=gsub("\\s+", "", person_ctry_code)) %>%
                                       mutate(person_ctry_code=ifelse(person_ctry_code=="" & !is.na(inferrediso2),inferrediso2,person_ctry_code))


test=lat_longs_mapbox %>% filter(person_ctry_code=="TW")



lat_longs_mapbox=lat_longs_mapbox %>% filter(!is.na(latitude) ) %>%
                                      filter(!is.na(person_ctry_code)) %>%
                                      filter(person_ctry_code!="") %>%
                                      filter(!(person_ctry_code!=inferrediso2 & !is.na(inferrediso2) ))



lat_longs_bycountry=lat_longs_mapbox %>% group_by(person_ctry_code,longitude,latitude) %>% summarise(innos=n())

```


### Capital city coordinates (used as fallback)

```{r}


# Create a comprehensive dataframe of capital cities with coordinates
capital_coords <- data.frame(
  iso2 = c(
    "AF", "AL", "DZ", "AD", "AO", "AG", "AR", "AM", "AU", "AT",
    "AZ", "BS", "BH", "BD", "BB", "BY", "BE", "BZ", "BJ", "BT",
    "BO", "BA", "BW", "BR", "BN", "BG", "BF", "BI", "KH", "CM",
    "CA", "CV", "CF", "TD", "CL", "CN", "CO", "KM", "CG", "CD",
    "CR", "CI", "HR", "CU", "CY", "CZ", "DK", "DJ", "DM", "DO",
    "EC", "EG", "SV", "GQ", "ER", "EE", "ET", "FJ", "FI", "FR",
    "GA", "GM", "GE", "DE", "GH", "GR", "GD", "GT", "GN", "GW",
    "GY", "HT", "HN", "HU", "IS", "IN", "ID", "IR", "IQ", "IE",
    "IL", "IT", "JM", "JP", "JO", "KZ", "KE", "KI", "KP", "KR",
    "KW", "KG", "LA", "LV", "LB", "LS", "LR", "LY", "LI", "LT",
    "LU", "MK", "MG", "MW", "MY", "MV", "ML", "MT", "MH", "MR",
    "MU", "MX", "FM", "MD", "MC", "MN", "ME", "MA", "MZ", "MM",
    "NA", "NR", "NP", "NL", "NZ", "NI", "NE", "NG", "NO", "OM",
    "PK", "PW", "PS", "PA", "PG", "PY", "PE", "PH", "PL", "PT",
    "QA", "RO", "RU", "RW", "KN", "LC", "VC", "WS", "SM", "ST",
    "SA", "SN", "RS", "SC", "SL", "SG", "SK", "SI", "SB", "SO",
    "ZA", "SS", "ES", "LK", "SD", "SR", "SZ", "SE", "CH", "SY",
    "TW", "TJ", "TZ", "TH", "TL", "TG", "TO", "TT", "TN", "TR",
    "TM", "TV", "UG", "UA", "AE", "GB", "US", "UY", "UZ", "VU",
    "VA", "VE", "VN", "YE", "ZM", "ZW"
  ),
  capital = c(
    "Kabul", "Tirana", "Algiers", "Andorra la Vella", "Luanda", "Saint John's", "Buenos Aires", "Yerevan", "Canberra", "Vienna",
    "Baku", "Nassau", "Manama", "Dhaka", "Bridgetown", "Minsk", "Brussels", "Belmopan", "Porto-Novo", "Thimphu",
    "Sucre", "Sarajevo", "Gaborone", "Brasilia", "Bandar Seri Begawan", "Sofia", "Ouagadougou", "Gitega", "Phnom Penh", "Yaounde",
    "Ottawa", "Praia", "Bangui", "N'Djamena", "Santiago", "Beijing", "Bogota", "Moroni", "Brazzaville", "Kinshasa",
    "San Jose", "Yamoussoukro", "Zagreb", "Havana", "Nicosia", "Prague", "Copenhagen", "Djibouti", "Roseau", "Santo Domingo",
    "Quito", "Cairo", "San Salvador", "Malabo", "Asmara", "Tallinn", "Addis Ababa", "Suva", "Helsinki", "Paris",
    "Libreville", "Banjul", "Tbilisi", "Berlin", "Accra", "Athens", "St. George's", "Guatemala City", "Conakry", "Bissau",
    "Georgetown", "Port-au-Prince", "Tegucigalpa", "Budapest", "Reykjavik", "New Delhi", "Jakarta", "Tehran", "Baghdad", "Dublin",
    "Jerusalem", "Rome", "Kingston", "Tokyo", "Amman", "Nur-Sultan", "Nairobi", "Tarawa", "Pyongyang", "Seoul",
    "Kuwait City", "Bishkek", "Vientiane", "Riga", "Beirut", "Maseru", "Monrovia", "Tripoli", "Vaduz", "Vilnius",
    "Luxembourg", "Skopje", "Antananarivo", "Lilongwe", "Kuala Lumpur", "Male", "Bamako", "Valletta", "Majuro", "Nouakchott",
    "Port Louis", "Mexico City", "Palikir", "Chisinau", "Monaco", "Ulaanbaatar", "Podgorica", "Rabat", "Maputo", "Naypyidaw",
    "Windhoek", "Yaren", "Kathmandu", "Amsterdam", "Wellington", "Managua", "Niamey", "Abuja", "Oslo", "Muscat",
    "Islamabad", "Ngerulmud", "Ramallah", "Panama City", "Port Moresby", "Asuncion", "Lima", "Manila", "Warsaw", "Lisbon",
    "Doha", "Bucharest", "Moscow", "Kigali", "Basseterre", "Castries", "Kingstown", "Apia", "San Marino", "Sao Tome",
    "Riyadh", "Dakar", "Belgrade", "Victoria", "Freetown", "Singapore", "Bratislava", "Ljubljana", "Honiara", "Mogadishu",
    "Pretoria", "Juba", "Madrid", "Colombo", "Khartoum", "Paramaribo", "Mbabane", "Stockholm", "Bern", "Damascus",
    "Taipei", "Dushanbe", "Dodoma", "Bangkok", "Dili", "Lome", "Nuku'alofa", "Port of Spain", "Tunis", "Ankara",
    "Ashgabat", "Funafuti", "Kampala", "Kyiv", "Abu Dhabi", "London", "Washington, D.C.", "Montevideo", "Tashkent", "Port Vila",
    "Vatican City", "Caracas", "Hanoi", "Sana'a", "Lusaka", "Harare"
  ),
  latitude = c(
    34.5553, 41.3275, 36.7538, 42.5063, -8.8383, 17.1175, -34.6037, 40.1792, -35.2809, 48.2082,
    40.4093, 25.0343, 26.0667, 23.8103, 13.0969, 53.9045, 50.8503, 17.2510, 6.4969, 27.4728,
    -19.0196, 43.8564, -24.6282, -15.8267, 4.8895, 42.6977, 12.3714, -3.3731, 11.5564, 3.8480,
    45.4215, 14.9177, 4.3947, 12.1348, -33.4489, 39.9042, 4.7110, -11.7172, -4.2634, -4.4419,
    9.9281, 6.8270, 45.8150, 23.1136, 35.1856, 50.0755, 55.6761, 11.8251, 15.3017, 18.4861,
    -0.1807, 30.0444, 13.6929, 3.7504, 15.3229, 59.4370, 9.0320, -18.1248, 60.1699, 48.8566,
    0.4162, 13.4549, 41.7151, 52.5200, 5.6037, 37.9838, 12.0561, 14.6349, 9.6412, 11.8637,
    6.8013, 18.5944, 14.0723, 47.4979, 64.1466, 28.6139, -6.2088, 35.6892, 33.3152, 53.3498,
    31.7683, 41.9028, 17.9714, 35.6762, 31.9454, 51.1694, -1.2921, 1.3382, 39.0392, 37.5665,
    29.3759, 42.8746, 17.9757, 56.9496, 33.8938, -29.3167, 6.3156, 32.8872, 47.1410, 54.6872,
    49.6116, 41.9973, -18.8792, -13.9626, 3.1390, 4.1755, 12.6392, 35.8989, 7.0897, 18.0735,
    -20.1609, 19.4326, 6.9147, 47.0105, 43.7384, 47.9186, 42.4304, 34.0209, -25.9655, 19.7633,
    -22.5597, -0.5477, 27.7172, 52.3676, -41.2865, 12.1150, 13.5127, 9.0765, 59.9139, 23.5880,
    33.6844, 7.5006, 31.9522, 8.9824, -9.4438, -25.2637, -12.0464, 14.5995, 52.2297, 38.7223,
    25.2854, 44.4268, 55.7558, -1.9403, 17.3026, 13.9094, 13.4877, -13.8333, 43.9424, 0.3364,
    24.7136, 14.7167, 44.7866, -4.6796, 8.4657, 1.3521, 48.1486, 46.0569, -9.4456, 2.0469,
    -25.7479, 4.8517, 40.4168, 6.9271, 15.5007, 5.8520, -26.3054, 59.3293, 46.9481, 33.5138,
    25.0330, 38.5598, -6.7924, 13.7563, -8.5569, 6.1256, -21.1789, 10.6918, 11.8745, 39.9334,
    37.9601, -8.5211, 0.3476, 50.4501, 24.4539, 51.5074, 38.9072, -34.9011, 41.2995, -17.7334,
    41.9029, 10.4806, 21.0285, 15.5527, -15.4167, -17.8252
  ),
  longitude = c(
    69.2075, 19.8187, 3.0588, 1.5218, 13.2344, -61.8456, -58.3816, 44.4991, 149.1300, 16.3738,
    49.8671, -77.3963, 50.5577, 90.4125, -59.6162, 27.5615, 4.3517, -88.7590, 2.6289, 89.6390,
    -65.2627, 18.4131, 25.9231, -47.9292, 114.9400, 23.3219, -1.5247, 29.8739, 104.9282, 11.5021,
    -75.6972, -23.5087, 18.5582, 15.0445, -70.6693, 116.4074, -74.0721, 43.2551, 15.2662, 15.2663,
    -84.0907, -5.2893, 15.9819, -82.3666, 33.3823, 14.4378, 12.5683, 43.1456, -61.3870, -69.9312,
    -78.4678, 31.2357, -89.2182, 8.7832, 38.9251, 24.7536, 38.7469, 178.4419, 24.9384, 2.3522,
    9.4673, -16.5790, 44.8271, 13.4050, -0.1870, 23.7275, -61.7480, -90.5069, -13.5784, -15.5989,
    -58.1551, -72.3074, -87.2068, 19.0402, -21.9426, 77.2090, 106.8456, 51.3890, 44.3661, -6.2603,
    35.2137, 12.4964, -76.7931, 139.6503, 35.9284, 71.4704, 36.8219, 172.9790, 125.7625, 126.9780,
    47.9774, 74.5698, 102.6000, 24.1052, 35.4953, 27.4974, -10.7969, 13.1913, 9.5215, 25.2797,
    6.1296, 21.4314, 47.5079, 33.7738, 101.6869, 73.5093, -8.0029, 14.5146, 171.1845, -15.9582,
    57.5012, -99.1332, 158.1611, 28.8497, 7.4246, 106.9057, 19.2594, -6.8498, 32.5732, 96.1561,
    17.0832, 166.9315, 85.3240, 4.9041, 174.7633, -86.2362, 2.1254, 7.4951, 10.7522, 58.4059,
    73.0479, 134.4893, 35.2332, -79.5199, 147.1803, -57.5759, -77.0428, 120.9842, 21.0122, -9.1393,
    51.5310, 26.1025, 37.6173, -1.9536, -62.7177, -60.9789, -61.2248, -171.7667, 12.4578, 6.7273,
    46.7160, -17.4467, 20.4489, 55.4540, -13.2317, 103.8198, 17.1077, 14.5058, 159.9729, 45.3182,
    28.1879, 31.5825, -3.7038, 79.8612, 32.5599, -55.2038, 31.1367, 18.0686, 7.4474, 36.2765,
    121.5654, 68.7870, 35.7516, 100.5018, 125.5603, 1.2313, -175.1982, -61.5089, 10.1815, 32.8597,
    58.3260, 179.1942, 32.5825, 30.5234, 54.3773, -0.1278, -77.0369, -56.1645, 69.2401, 168.3273,
    12.4534, -66.8792, 105.8542, 44.2070, 28.2871, 31.0522
  ),
  stringsAsFactors = FALSE
)

# View the dataframe
head(capital_coords, 10)

# Optional: Sort by ISO2 code
capital_coords <- capital_coords[order(capital_coords$iso2), ]

# Print summary
cat("Total countries:", nrow(capital_coords), "\n")

```


### Assemble dfnew

```{r}
capital_coords <- capital_coords %>% rename(person_ctry_code = iso2)


anti=df %>% anti_join(lat_longs_mapbox) %>% select(person_id,person_ctry_code)          # figure out addresses that are not in the mapbox frame
anti=anti %>% filter(!is.na(person_ctry_code) & gsub("\\s+", "", person_ctry_code)!="") # We focus only on those where we have at least a country code


lat_long=lat_longs_mapbox %>% select(longitude,latitude,person_ctry_code) %>% bind_rows(capital_coords) %>% group_by(person_ctry_code) %>%
  mutate(rr=runif(1:n())) %>% arrange(person_ctry_code, rr) %>% mutate(nn=1:n()) %>% filter(nn<3) %>%  # Let's restrict to 5 places per country...
  select(longitude,latitude,person_ctry_code)


anti=anti %>% left_join(lat_long,by="person_ctry_code") %>%
  group_by(person_id) %>% mutate(rr=runif(1:n())) %>% arrange(person_id, rr) %>% mutate(nn=1:n()) %>% filter(nn==1)


anti=anti %>% select(-nn,-rr)

dfnew=anti %>% bind_rows(lat_longs_mapbox) %>% select(person_id, person_ctry_code,longitude,latitude)

```


### Saving dfnew

```{r}
write_parquet(dfnew,paste0(inglobebig,"\\dfnew.parquet"))
write_parquet(dfnew,paste0(inglobebig,"\\data\\dfnew.parquet"))

```

